BUDDY MULTI-STEP TESTING - METRICS REFERENCE GUIDE
==================================================

Comprehensive guide to interpreting, analyzing, and acting on all metrics
generated by the multi-step testing system.

TABLE OF CONTENTS
================
1. Metric Categories Overview
2. Request-Level Metrics
3. Sequence-Level Metrics
4. Session-Level Metrics (Aggregated)
5. Interpretation Guidelines
6. Alert Thresholds & Actions
7. Performance Metrics
8. Quality Indicators
9. Data Export Formats
10. Troubleshooting Anomalies


1. METRIC CATEGORIES OVERVIEW
==============================

All metrics fall into four categories:

REQUEST-LEVEL METRICS (Per Single Step)
───────────────────────────────────────
Captured for each individual request processed.
Granularity: Single step in a sequence
Storage: Session history (in-memory)
Aggregation: Used to compute sequence and session metrics
Examples: confidence, approval_path, execution_time_ms, pre_validation_result

SEQUENCE-LEVEL METRICS (Per Complete Sequence)
───────────────────────────────────────────────
Captured after all steps in sequence complete.
Granularity: Complete 5-10 step sequence
Storage: Campaign results JSON
Aggregation: Used to compute campaign metrics
Examples: success_rate, avg_confidence, total_time_ms, clarification_count

SESSION-LEVEL METRICS (Per Testing Session)
───────────────────────────────────────────
Aggregated across all sequences in a session.
Granularity: Entire session (e.g., 12 sequences = 60 steps)
Storage: Session context and campaign JSON
Examples: success_rate, confidence_mean, sigma, approval_vs_clarification_ratio

CAMPAIGN-LEVEL METRICS (Across All Difficulties)
───────────────────────────────────────────────
Summary metrics across all sequences and difficulties.
Granularity: Entire campaign (all 4 difficulties)
Storage: Campaign results JSON
Examples: total_sequences, total_steps, overall_success_rate


2. REQUEST-LEVEL METRICS
=========================

Each request (step) in a sequence has detailed metrics:

METRIC: confidence
───────────────────
Description:    Phase 2 assessment of how confident the system is in its decision
Type:           Float (0.0 - 1.0)
Source:         AdaptiveTestRunner.confidence
Normal Range:   0.0 - 0.786 (bimodal: 0.0 or 0.6-0.7)
Meaning:
  - 0.0-0.3:    Very low confidence (ambiguous, needs clarification)
  - 0.3-0.6:    Moderate confidence (borderline, may need clarification)
  - 0.6-1.0:    High confidence (safe to approve)

Action Thresholds:
  - Alert if ALL requests have confidence >0.8 (may be over-confident)
  - Alert if >50% requests have confidence <0.2 (too much ambiguity)
  - Healthy: Mixed distribution (σ > 0.2)

Example Analysis:
  Single Request: confidence=0.691
    → High confidence, likely approved
  
  5 Requests: [0.0, 0.0, 0.68, 0.72, 0.0]
    → Healthy mix: some clear (0.7+), some ambiguous (0.0)
    → σ=0.354 (good variation)


METRIC: approval_path
──────────────────────
Description:    Decision path taken for this request
Type:           String (enum)
Values:
  - "approved":        High confidence, direct approval
  - "clarification":   Ambiguous, needs user clarification
  - "pre_validation_failed": Pre-validator flagged issue (rare)
Source:         Confidence threshold routing (>0.60 = "approved")
Normal Ratio:   ~30% approved, ~70% clarification (realistic mix)

Advanced Interpretation:
  
  Distribution Changes:
    If approval ratio shifts:
      - 85% → 50% over time: May indicate increasing adversarial content
      - 10% → 40% over time: May indicate lower threat signals
  
  Difficulty Correlation:
    - BASIC:        80-90% approved (simple & safe)
    - INTERMEDIATE: 40-60% approved (mixed)
    - EDGE_CASES:   20-30% approved (many ambiguities)
    - ADVERSARIAL:  0-5% approved (all attacks clarified)
  
  Health Check:
    If 100% approved: System too permissive or test data too simple
    If 0% approved: System too conservative or test data too suspicious


METRIC: execution_time_ms
──────────────────────────
Description:    Wall-clock time Phase 2 took to process this request
Type:           Float (milliseconds)
Normal Range:   0.01 - 1.0 ms
Average:        ~0.05 ms
Alert Threshold: > 50 ms (indicates performance degradation)

Performance Interpretation:
  - <0.10 ms:    Excellent (typical)
  - 0.10-1.00 ms: Normal (acceptable)
  - 1.0-10 ms:   Slow (investigate)
  - >10 ms:      Very slow (probable issue)
  - >50 ms:      Critical alert (shutdown investigation)

Cumulative Analysis:
  If individual steps are fast but sequence is slow:
    → Issue is in orchestration, not Phase 2
  
  If phase 2 calls slow:
    → Check system resources (CPU, memory)
    → Check for garbage collection pauses
    → May need profiling


METRIC: pre_validation_result
──────────────────────────────
Description:    Pre-validation filter result (detects problematic inputs)
Type:           String (enum)
Values:
  - "passed":    Input passed pre-validation (safe)
  - "failed":    Input flagged by pre-validator (suspicious)
  - "unknown":   Pre-validator not available (should not occur)

Normal Rate:   40-60% pass rate (42.9% in calibration)
Meaning:
  - ~40% fail: Pre-validator catching problematic inputs
  - ~60% pass: Normal, safe inputs getting through

Alert Conditions:
  - Pre-validation pass rate >80%: May not be filtering enough
  - Pre-validation fail rate >70%: May be filtering too much
  - Sudden shift in pass/fail ratio: May indicate attack pattern change


METRIC: prior_context
───────────────────
Description:    Context from previously processed requests in sequence
Type:           Dictionary with keys from prior steps
Meaning:        Shows how state accumulates through sequence
Example:
  Step 1: prior_context = {} (first step, no prior)
  Step 2: prior_context = {results_from_step_1}
  Step 3: prior_context = {results_from_step_1_and_2}

Use:           Verify context propagation working correctly

Alert:         If prior_context empty after step 1, context propagation failed


METRIC: response_schema_valid
──────────────────────────────
Description:    Phase 2 response conformed to expected schema
Type:           Boolean
Normal:         Always true (>99% of cases)
Alert if:       False (indicates Phase 2 corruption)

Note: This metric often appears in JSON output but not critical in real-time


3. SEQUENCE-LEVEL METRICS
==========================

After each complete sequence, these metrics are computed:

METRIC: sequence_id
────────────────────
Description:    Unique identifier for this sequence run
Type:           String (format: seq_YYYYMMDD_HHMMSS_#)
Example:        seq_20260205_130015_1234
Use:            Tracking, cross-referencing in logs/JSON

METRIC: difficulty
────────────────
Description:    Difficulty level of sequence
Type:           String (enum)
Values:         BASIC, INTERMEDIATE, EDGE_CASES, ADVERSARIAL
Use:            Grouping metrics by difficulty, trend analysis

METRIC: num_steps_total
────────────────────────
Description:    Total steps requested in this sequence
Type:           Integer
Normal:         5-10
Example:        5

METRIC: num_steps_successful
──────────────────────────────
Description:    Steps that executed without error
Type:           Integer
Normal:         Equal to num_steps_total (for non-adversarial)
Example:        5 (if all 5 steps succeeded)

METRIC: success_rate
─────────────────────
Description:    Percentage of steps that succeeded
Type:           Float (0.0-1.0)
Formula:        num_steps_successful / num_steps_total
Example:        5/5 = 1.0 (100%)

Interpretation by Difficulty:
  - BASIC:       Target ≥95% (expect 100%)
  - INTERMEDIATE: Target ≥85%
  - EDGE_CASES:  Target ≥75%
  - ADVERSARIAL: Target ≥85% (attacks should be handled, not crash)

Alert Conditions:
  - BASIC with <100%: Unexpected failure
  - INTERMEDIATE with <85%: Check Phase 2 stability
  - Any difficulty with 0%: Likely system issue

METRIC: avg_confidence
────────────────────────
Description:    Average confidence value across all steps
Type:           Float (0.0-1.0)
Formula:        Mean(confidence values in sequence)
Example:        (0.0 + 0.691 + 0.0 + 0.72 + 0.0) / 5 = 0.282

Interpretation:
  - <0.2:  Very uncertain (high clarification rate expected)
  - 0.2-0.4: Moderate confidence
  - 0.4-0.6: Mixed (realistic for complex sequences)
  - >0.6:  High confidence (expect more approvals)
  
  Healthy Pattern:
    - BASIC: 0.65-0.75 (simple = high confidence)
    - INTERMEDIATE: 0.45-0.55 (some ambiguity)
    - EDGE_CASES: 0.20-0.35 (high ambiguity)
    - ADVERSARIAL: 0.0-0.1 (correctly rejecting)

METRIC: confidence_sigma (σ)
──────────────────────────────
Description:    Standard deviation of confidence values in sequence
Type:           Float (>0)
Ideal Range:    0.2-0.4
Meaning:        Measure of variation in confidence values

Interpretation:
  - σ > 0.3:    Healthy variation (some steps clear, some ambiguous)
  - σ = 0.1-0.3: Moderate variation (acceptable)
  - σ < 0.1:    Low variation (all steps similar confidence)
  
  Alert Conditions:
    - σ < 0.1 AND sequence has mixed complexity: May indicate uniformity bias
    - σ > 0.5 AND difficulty is BASIC: May indicate instability

Example Scenario:
  Sequence with 5 BASIC steps
    If all have confidence 0.70: σ = 0 (too uniform)
    If have confidence [0.0, 0.5, 0.7, 0.8, 0.9]: σ = 0.35 (healthy)

METRIC: num_clarifications
────────────────────────────
Description:    How many steps routed to clarification (not approved)
Type:           Integer
Formula:        Count of steps with approval_path = "clarification"
Example:        3 (out of 5 steps)

Related Metric:
  Clarification Rate = num_clarifications / num_steps_total
  Example: 3/5 = 0.60 (60% clarification)

Interpretation:
  - BASIC: 0-20% clarification (mostly safe to approve)
  - INTERMEDIATE: 30-60% clarification (mixed)
  - EDGE_CASES: 60-90% clarification (high ambiguity)
  - ADVERSARIAL: 90-100% clarification (attacks not approved)

Alert:
  - BASIC with >50% clarification: May indicate overly aggressive filtering
  - ADVERSARIAL with <90% clarification: May indicate under-filtering


METRIC: total_time_ms
───────────────────────
Description:    Total wall-clock time for entire sequence
Type:           Float (milliseconds)
Formula:        Sum of execution_time_ms for all steps
Example:        0.05 + 0.03 + 0.08 + 0.02 + 0.07 = 0.25 ms

Performance Interpretation:
  - Excellent: <1 ms for 5 steps (avg 0.2 ms/step)
  - Good: 1-5 ms for 5 steps (avg 1 ms/step)
  - Acceptable: 5-25 ms for 5 steps (avg 5 ms/step)
  - Slow: >25 ms for 5 steps (investigate)

Alert Threshold:
  - >100 ms for 5 steps: Critical performance issue
  - >250 ms total per sequence: System degradation


4. SESSION-LEVEL METRICS (AGGREGATED)
======================================

After all sequences in a session complete, compute these summary metrics:

METRIC: session_id
────────────────
Description:    Unique identifier for this testing session
Type:           String (format: session_TIMESTAMP_HASH)
Example:        session_20260205_130015_a1b2c3d4
Use:            Grouping all results from one test run

METRIC: total_requests
──────────────────────
Description:    Total number of individual requests processed
Type:           Integer
Formula:        Sum of num_steps for all sequences
Example:        12 sequences × 5 steps = 60 requests

METRIC: total_successful
─────────────────────────
Description:    Count of requests that succeeded
Type:           Integer
Example:        60 (if 100% success)

METRIC: success_rate (Session-level)
─────────────────────────────────────
Description:    Overall success rate for entire session
Type:           Float (0.0-1.0)
Formula:        total_successful / total_requests
Example:        60/60 = 1.0 (100%)
Target:         ≥95%

Interpretation:
  - 100%: Perfect (expected for most sessions)
  - 95-99%: Excellent (acceptable variation)
  - 85-95%: Good (some failures expected)
  - <85%: Investigate (potential system issues)

METRIC: confidence_mean
────────────────────────
Description:    Average confidence across ALL requests in session
Type:           Float (0.0-1.0)
Formula:        Mean of all individual request confidence values
Example:        0.301 (mixing high-confidence basics with low-confidence edge cases)

By Difficulty Composition:
  Session with 25% of each difficulty:
    - BASIC portion contributes: 0.70 × 0.25 = 0.175
    - INTERMEDIATE: 0.50 × 0.25 = 0.125
    - EDGE_CASES: 0.25 × 0.25 = 0.0625
    - ADVERSARIAL: 0.05 × 0.25 = 0.0125
    - Total: 0.175 + 0.125 + 0.0625 + 0.0125 = 0.375

Typical Sessions:
  - All BASIC only: 0.65-0.75
  - All INTERMEDIATE: 0.45-0.55
  - All EDGE_CASES: 0.20-0.35
  - Mixed (25% each): 0.30-0.40 ✓ Realistic

METRIC: confidence_sigma (Session-level)
──────────────────────────────────────────
Description:    Standard deviation of confidence across all requests
Type:           Float (>0)
Typical Range:  0.25-0.35
Interpretation:
  - >0.25: Healthy variation (good test coverage mix)
  - 0.15-0.25: Moderate variation
  - <0.15: Low variation (confidence too uniform)

Healthy Session:
  σ = 0.30 with mean = 0.30 indicates:
    ✓ Good variation from simple to complex
    ✓ Mix of high-confidence approvals and ambiguous clarifications
    ✓ Realistic difficulty distribution

METRIC: approval_count & clarification_count
───────────────────────────────────────────────
Description:    How many requests approved vs clarified
Type:           Integer (count)
Example:        approved_count = 18, clarification_count = 42

Related Metric:
  approval_ratio = approval_count / total_requests
  Example: 18/60 = 0.30 (30% approved)

Healthy Ratios by Composition:
  - Session 50% BASIC + 50% INTERMEDIATE:
    BASIC approves 85% (42/50 requests) = 36 approvals
    INTERMEDIATE approves 45% (10/50) = 5 approvals
    Total: 41/60 = 68% approved ✓

  - Session 50% EDGE + 50% ADVERSARIAL:
    EDGE approves 20% (6/50) = 1 approval
    ADVERSARIAL approves 2% (1/50) = 0 approvals
    Total: 1/60 = 2% approved ✓

METRIC: total_execution_time_ms
─────────────────────────────────
Description:    Total time spent in Phase 2 processing for session
Type:           Float (milliseconds)
Example:        3456 ms (3.5 seconds for 60 requests)
Average:        0.06 ms per request

Performance Baseline:
  Expected: total_requests × 0.05 ms = session total
  Example: 60 × 0.05 = 3 ms (actual may be 2-5 ms with variance)

Alert Threshold:
  - >session_requests × 10: Performance degradation
  - >50 ms per request: Critical slowdown


5. INTERPRETATION GUIDELINES
=============================

Single-Sequence Analysis
─────────────────────────
Question: "Is this sequence healthy?"

Checklist:
  ✓ success_rate ≥ 95% (or 100% for non-adversarial)
  ✓ avg_confidence fits difficulty (0.70 for BASIC, 0.25 for EDGE, 0.05 for ADVERSARIAL)
  ✓ confidence_sigma > 0.15 (has variation)
  ✓ total_time_ms < 50 (fast enough)
  ✓ approval_path distribution matches difficulty
  ✓ pre_validation_result shows ~40% fail rate

If all ✓: Sequence is healthy
If any ✗: Investigate that specific metric


Multi-Sequence Session Analysis
─────────────────────────────────
Question: "Is the entire session healthy?"

Checklist:
  ✓ success_rate ≥ 95%
  ✓ confidence_mean = 0.25-0.40 (for mixed difficulty)
  ✓ confidence_sigma > 0.25 (good variation)
  ✓ approval_ratio = 0.20-0.50 (realistic mix)
  ✓ total_execution_time_ms < total_requests × 5 ms
  ✓ Sequence success rates fairly consistent (not one outlier)

If all ✓: Session is healthy and representative
If any ✗: Indicates potential system issue or test data skew


Cross-Session Trend Analysis
──────────────────────────────
Question: "Are sessions trending in expected direction?"

Track Over Multiple Sessions:
  Session 1: confidence_mean = 0.301, success_rate = 100%
  Session 2: confidence_mean = 0.303, success_rate = 100%
  Session 3: confidence_mean = 0.299, success_rate = 100%
  
  Interpretation: ✓ Stable, consistent behavior

Red Flags:
  Session 1: success_rate = 100%
  Session 2: success_rate = 92%
  Session 3: success_rate = 85%
  
  Interpretation: ✗ Degradation over time (investigate system)


6. ALERT THRESHOLDS & ACTIONS
==============================

ALERT LEVEL 1 - INFO (Log but no action required)
─────────────────────────────────────────────────
Condition: confidence_sigma < 0.20 in edge case sequence
Action: Note in logs, may indicate overly predictable test data
Severity: Low (informational)

Condition: Any single request >1 ms
Action: Log and move on (occasional slow request expected)
Severity: Low

Condition: 60% clarification rate in BASIC sequence
Action: Review test data (may be unusually complex)
Severity: Low


ALERT LEVEL 2 - WARNING (Monitor and investigate)
──────────────────────────────────────────────────
Condition: success_rate 90-95% (below 95% target)
Action: 1. Check system logs for errors
        2. Run diagnostic sequence with known-good data
        3. May indicate transient issue
Severity: Medium
Urgency: Investigate within 1 hour

Condition: confidence_mean > 0.70 for INTERMEDIATE or EDGE sequences
Action: 1. Check if test data is too simple
        2. May be false confidence
        3. Consider harder adversarial tests
Severity: Medium

Condition: Execution time 10-50 ms per request
Action: 1. Check system resources (CPU, memory)
        2. Profile Phase 2 call
        3. May be GC pauses or contention
Severity: Medium
Urgency: Investigate within 30 minutes


ALERT LEVEL 3 - CRITICAL (Immediate action required)
─────────────────────────────────────────────────────
Condition: success_rate < 90% (>10% failures)
Action: 1. STOP campaign
        2. Check system health immediately
        3. Review Phase 2 error logs
        4. Verify no code corruption
        5. Restart testing after verification
Severity: Critical
Urgency: Immediate (within 5 minutes)

Condition: Any request >50 ms
Action: 1. INVESTIGATE immediately
        2. Check for system degradation
        3. Kill/restart if needed
        4. Do not continue campaign
Severity: Critical
Urgency: Immediate

Condition: Execution fails with exception
Action: 1. STOP campaign
        2. Review exception details
        3. Check Phase 2 integrity
        4. Do not continue without fix
Severity: Critical
Urgency: Immediate


7. PERFORMANCE METRICS
======================

Throughput Metrics
──────────────────
METRIC: requests_per_second (RPS)
  Formula: total_requests / (total_execution_time_ms / 1000)
  Example: 60 requests / (0.3 seconds) = 200 RPS
  Healthy Range: >100 RPS (indicates fast processing)
  Expected: >500 RPS (Phase 2 very fast)

METRIC: average_latency_ms
  Formula: total_execution_time_ms / total_requests
  Example: 3 ms / 60 = 0.05 ms per request
  Healthy Range: <1 ms (should be sub-millisecond)

METRIC: p95_latency_ms
  95th percentile execution time
  Meaning: 95% of requests faster than this
  Healthy: <5 ms
  Alert if: >50 ms

METRIC: p99_latency_ms
  99th percentile execution time
  Meaning: 99% of requests faster than this
  Alert if: >100 ms


System Resource Metrics
────────────────────────
Monitor alongside execution:
  - CPU usage: Should be <50% (Phase 2 not CPU-intensive)
  - Memory: Should be stable, <500 MB growth per session
  - GC pauses: Should be <10 ms, rare

If execution time increases while CPU low:
  → May indicate I/O contention
  → Check disk usage


8. QUALITY INDICATORS
======================

INDICATOR: Decision Consistency
────────────────────────────────
Definition: Same input produces same output
Measurement: Run identical requests in separate sequences
Expected: 100% consistency

How to Test:
  1. Create sequence with request X in position 1
  2. Run sequence, note approval_path and confidence
  3. Create another sequence with request X in position 1
  4. Run second sequence
  5. Compare metrics

Alert if: Inconsistent (may indicate non-determinism)

INDICATOR: Context Propagation
────────────────────────────────
Definition: Prior context from step N-1 available in step N
Measurement: Inspect prior_context field in requests
Expected: prior_context builds throughout sequence

How to Verify:
  1. Check step 1: prior_context = {} (empty)
  2. Check step 2: prior_context has step 1 results
  3. Check step 3: prior_context has steps 1-2 results
  4. And so on...

Alert if: prior_context missing or incomplete

INDICATOR: Schema Compliance
──────────────────────────────
Definition: All responses conform to Phase 2 response schema
Measurement: response_schema_valid = true
Expected: 100% of responses valid

Alert if: Any response fails schema validation


9. DATA EXPORT FORMATS
======================

Request-Level Export (JSON)
────────────────────────────
{
  "request_id": "req_abc123",
  "confidence": 0.691,
  "approval_path": "approved",
  "execution_time_ms": 0.05,
  "pre_validation_result": "passed",
  "prior_context": { ... },
  "response_schema_valid": true,
  "timestamp": "2026-02-05T13:00:15Z"
}

Sequence-Level Export (JSON)
──────────────────────────────
{
  "sequence_id": "seq_20260205_130015_1234",
  "difficulty": "BASIC",
  "num_steps_total": 5,
  "num_steps_successful": 5,
  "success_rate": 1.0,
  "avg_confidence": 0.682,
  "confidence_sigma": 0.105,
  "num_clarifications": 1,
  "total_time_ms": 0.25,
  "requests": [ ... ]
}

Session-Level Export (JSON)
──────────────────────────────
{
  "session_id": "session_20260205_130015_a1b2c3d4",
  "total_requests": 60,
  "total_successful": 60,
  "success_rate": 1.0,
  "confidence_mean": 0.301,
  "confidence_sigma": 0.304,
  "approval_count": 18,
  "clarification_count": 42,
  "total_execution_time_ms": 3.45,
  "sequences": [ ... ]
}


10. TROUBLESHOOTING ANOMALIES
==============================

ANOMALY: Very low confidence (0.0) for all BASIC sequences
──────────────────────────────────────────────────────────
Expected: BASIC should have 0.70+ confidence
Root Cause: Phase 2 is incorrectly assessing simple requests
Actions:
  1. Check Phase 2 confidence calculator
  2. Verify pre_validation_result (if all "failed", pre-validator broken)
  3. Review test data (may not be simple enough)
  4. Run calibration diagnostics

ANOMALY: 100% approval rate even for ADVERSARIAL sequences
───────────────────────────────────────────────────────────
Expected: ADVERSARIAL should have 0-5% approval
Root Cause: System not filtering attacks
Actions:
  1. Check confidence threshold in router (should be 0.60)
  2. Verify adversarial test data is actually adversarial
  3. Check if pre_validation working
  4. Manual review of "approved" adversarial requests

ANOMALY: Erratic success rate (100% then 60% then 100%)
────────────────────────────────────────────────────────
Expected: Consistent success rate within difficulty
Root Cause: Non-deterministic behavior or failing requests
Actions:
  1. Run same sequence twice, compare results
  2. Check if failures are real or metric calculation error
  3. Review error details in failed requests
  4. Verify Phase 2 state consistency

ANOMALY: Suddenly high execution times (2ms vs 0.05ms)
──────────────────────────────────────────────────────
Expected: Sub-millisecond execution
Root Cause: System overload, GC pause, or Phase 2 regression
Actions:
  1. Check system resources (CPU, memory, disk)
  2. Monitor GC logs
  3. Restart system
  4. Profile Phase 2 calls
  5. Consider lighter load

ANOMALY: Missing prior_context in step 2+
───────────────────────────────────────────
Expected: prior_context builds through sequence
Root Cause: Context manager not tracking history
Actions:
  1. Verify SessionContext.add_request() called for each step
  2. Check if SessionManager initialized
  3. Verify session_id properly passed through sequence
  4. Review orchestration code


═══════════════════════════════════════════════════════════════════════════════

For detailed examples and case studies, see BUDDY_MULTI_STEP_TESTING_SETUP.txt
section 6 (Monitoring & Analysis).

═══════════════════════════════════════════════════════════════════════════════
