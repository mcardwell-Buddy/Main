"""
Phase 12: Autonomous Strategic Execution Harness
Orchestrates strategic workflow execution with adaptive decision-making based on Phase 11 learning.
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
import argparse

from buddy_dynamic_task_scheduler import TaskScheduler, TaskPriority, RiskLevel
from phase2_confidence import GradedConfidenceCalculator, ConfidenceFactors
from buddy_learning_analyzer import LearningAnalyzer
from buddy_autonomous_goal_generator import AutonomousGoalGenerator
from buddy_policy_updater import PolicyUpdater, PolicyState
from buddy_strategic_executor import StrategicExecutor

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class StrategicHarness:
    """
    Autonomous strategic execution harness for Phase 12.
    
    Ingests Phase 11 learning insights and executes strategic workflows
    with adaptive decision-making, confidence elevation, and alternate path selection.
    """
    
    def __init__(self, phase11_dir: Path, output_dir: Path):
        self.phase11_dir = Path(phase11_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize components
        self.scheduler = TaskScheduler(
            max_concurrent_tasks=1,
            enable_dry_run=True
        )
        self.confidence_calculator = GradedConfidenceCalculator()
        
        # Load Phase 11 artifacts
        self.policy = self._load_phase11_policy()
        self.learning_insights = self._load_phase11_insights()
        
        # Initialize analyzers and generators
        # LearningAnalyzer can analyze Phase 11 outputs (same structure as Phase 10)
        self.analyzer = LearningAnalyzer(Path(str(self.phase11_dir)))
        self.goal_generator = AutonomousGoalGenerator(self.analyzer)
        self.policy_updater = PolicyUpdater(self.policy)
        
        # Register scheduler actions
        self._register_scheduler_actions()
        
        # Tracking
        self.all_outcomes = []
        self.all_questions = []
        self.all_confidence_updates = []
        self.all_policy_updates = []
        self.all_strategic_decisions = []
        self.wave_stats = []
    
    def _load_phase11_policy(self) -> PolicyState:
        """Load final policy state from Phase 11."""
        ui_state_file = self.phase11_dir / "phase11_ui_state.json"
        if not ui_state_file.exists():
            logger.warning(f"Phase 11 UI state not found at {ui_state_file}, using defaults")
            return PolicyState()
        
        with open(ui_state_file) as f:
            ui_state = json.load(f)
        
        policy_data = ui_state.get("policy", {})
        return PolicyState(
            high_risk_threshold=policy_data.get("high_risk_threshold", 0.8),
            retry_multiplier=policy_data.get("retry_multiplier", 1.0),
            priority_bias=policy_data.get("priority_bias", 1.0)
        )
    
    def _load_phase11_insights(self) -> List[Dict[str, Any]]:
        """Load learning insights from Phase 11."""
        insights_file = self.phase11_dir / "learning_insights.jsonl"
        if not insights_file.exists():
            logger.warning(f"Learning insights not found at {insights_file}")
            return []
        
        insights = []
        with open(insights_file) as f:
            for line in f:
                insights.append(json.loads(line.strip()))
        
        logger.info(f"Loaded {len(insights)} learning insights from Phase 11")
        return insights
    
    def _register_scheduler_actions(self):
        """Register web actions with scheduler."""
        def make_action(name: str):
            def _action(**kwargs):
                return {"dry_run": True, "action": name, "params": kwargs}
            return _action
        
        self.scheduler.register_action("web_inspect", make_action("web_inspect"))
        self.scheduler.register_action("web_extract", make_action("web_extract"))
        self.scheduler.register_action("web_click", make_action("web_click"))
        self.scheduler.register_action("web_fill", make_action("web_fill"))
        self.scheduler.register_action("high_risk_submit", make_action("high_risk_submit"))
    
    def run(self, waves: int = 3):
        """
        Execute strategic harness for specified number of waves.
        
        Args:
            waves: Number of waves to execute
        """
        logger.info(f"=== Phase 12 Strategic Execution: {waves} waves ===")
        
        for wave_num in range(1, waves + 1):
            logger.info(f"\n--- Wave {wave_num} ---")
            
            # Generate strategic goals based on Phase 11 learning
            goals = self.goal_generator.generate_goals(wave_num)
            logger.info(f"Generated {len(goals)} strategic goals")
            
            # Convert goals to task specs
            tasks = []
            for goal in goals:
                for task_spec in goal.tasks:  # AutonomousGoal.tasks is List[TaskSpec]
                    tasks.append({
                        "task_id": task_spec.task_id,
                        "description": task_spec.title,  # TaskSpec uses 'title'
                        "action": task_spec.tool,  # TaskSpec uses 'tool'
                        "action_params": task_spec.metadata.get("params", {}),
                        "priority": task_spec.priority,
                        "risk_level": task_spec.risk,  # TaskSpec uses 'risk'
                        "confidence_score": task_spec.confidence,
                        "dependencies": task_spec.dependencies
                    })
            
            # Execute wave with strategic decision-making
            executor = StrategicExecutor(
                self.output_dir / f"wave_{wave_num}",
                self.learning_insights,
                wave_num
            )
            
            outcomes, questions, updates = executor.execute_wave(
                tasks,
                f"phase12_wave_{wave_num}"
            )
            
            # Collect strategic decisions
            strategic_summary = executor.get_strategic_summary()
            self.all_strategic_decisions.extend(strategic_summary["decisions"])
            
            # Update policy based on outcomes
            policy_update = self.policy_updater.update_from_outcomes(outcomes)
            
            # Track aggregates
            self.all_outcomes.extend(outcomes)
            self.all_questions.extend(questions)
            self.all_confidence_updates.extend(updates)
            self.all_policy_updates.append(policy_update)
            
            # Track wave stats
            wave_stat = {
                "wave": wave_num,
                "total_tasks": len(tasks),
                "completed": sum(1 for o in outcomes if o["status"] == "completed"),
                "failed": sum(1 for o in outcomes if o["status"] == "failed"),
                "deferred": sum(1 for o in outcomes if o["status"] == "deferred"),
                "success_rate": sum(1 for o in outcomes if o["status"] == "completed") / len(tasks) if tasks else 0,
                "strategic_goals": len(goals),
                "strategic_decisions": strategic_summary["total_decisions"],
                "confidence_boost_avg": strategic_summary["avg_confidence_boost"]
            }
            self.wave_stats.append(wave_stat)
            
            logger.info(f"Wave {wave_num}: {wave_stat['completed']}/{wave_stat['total_tasks']} completed, "
                       f"{wave_stat['strategic_decisions']} strategic decisions, "
                       f"avg boost: {wave_stat['confidence_boost_avg']:.3f}")
        
        # Write aggregate outputs
        self._write_aggregate_logs()
        self._write_ui_state()
        self._write_report()
        
        logger.info(f"\nPhase 12 complete. Logs saved to {self.output_dir}")
    
    def _write_aggregate_logs(self):
        """Write aggregate JSONL logs."""
        # Self-questions
        with open(self.output_dir / "self_questions.jsonl", "w") as f:
            for q in self.all_questions:
                f.write(json.dumps(q) + "\n")
        
        # Task outcomes
        with open(self.output_dir / "task_outcomes.jsonl", "w") as f:
            for o in self.all_outcomes:
                f.write(json.dumps(o) + "\n")
        
        # Confidence updates
        with open(self.output_dir / "confidence_updates.jsonl", "w") as f:
            for u in self.all_confidence_updates:
                f.write(json.dumps(u) + "\n")
        
        # Policy updates
        with open(self.output_dir / "policy_updates.jsonl", "w") as f:
            for p in self.all_policy_updates:
                f.write(json.dumps(p) + "\n")
        
        # Strategic decisions
        with open(self.output_dir / "strategic_decisions.jsonl", "w") as f:
            for d in self.all_strategic_decisions:
                f.write(json.dumps(d) + "\n")
    
    def _write_ui_state(self):
        """Write UI state for Phase 7/8 observability."""
        ui_state = {
            "generated_at": datetime.utcnow().isoformat(),
            "wave_stats": self.wave_stats,
            "policy": {
                "high_risk_threshold": self.policy_updater.policy.high_risk_threshold,
                "retry_multiplier": self.policy_updater.policy.retry_multiplier,
                "priority_bias": self.policy_updater.policy.priority_bias
            },
            "learning_insights": self.learning_insights,
            "strategic_summary": {
                "total_decisions": len(self.all_strategic_decisions),
                "by_type": self._count_decisions_by_type(),
                "avg_confidence_boost": self._avg_confidence_boost()
            }
        }
        
        with open(self.output_dir / "phase12_ui_state.json", "w") as f:
            json.dump(ui_state, f, indent=2)
    
    def _count_decisions_by_type(self) -> Dict[str, int]:
        """Count strategic decisions by type."""
        counts = {}
        for decision in self.all_strategic_decisions:
            dtype = decision.get("decision_type", "unknown")
            counts[dtype] = counts.get(dtype, 0) + 1
        return counts
    
    def _avg_confidence_boost(self) -> float:
        """Calculate average confidence boost."""
        if not self.all_strategic_decisions:
            return 0.0
        total = sum(
            d.get("confidence_after", 0) - d.get("confidence_before", 0)
            for d in self.all_strategic_decisions
        )
        return total / len(self.all_strategic_decisions)
    
    def _write_report(self):
        """Generate strategic execution report."""
        report_lines = [
            "# Phase 12 Strategic Execution Report â€” Autonomous Adaptation",
            "",
            f"Generated: {datetime.utcnow().isoformat()}",
            "",
            "## Executive Summary",
            f"Phase 12 ingested Phase 11 learning insights and executed {len(self.wave_stats)} waves of strategic workflows.",
            f"Applied {len(self.all_strategic_decisions)} strategic decisions with adaptive confidence elevation and policy tuning.",
            "",
            "## Strategic Decision Summary",
            f"- **Total Decisions:** {len(self.all_strategic_decisions)}",
            f"- **Average Confidence Boost:** {self._avg_confidence_boost():.3f}",
            "- **Decision Types:**"
        ]
        
        for dtype, count in self._count_decisions_by_type().items():
            report_lines.append(f"  - {dtype}: {count}")
        
        report_lines.extend([
            "",
            "## Wave Statistics"
        ])
        
        for stat in self.wave_stats:
            report_lines.extend([
                f"- Wave {stat['wave']}: total={stat['total_tasks']}, completed={stat['completed']}, "
                f"failed={stat['failed']}, deferred={stat['deferred']}, "
                f"strategic_decisions={stat['strategic_decisions']}, "
                f"confidence_boost_avg={stat['confidence_boost_avg']:.3f}"
            ])
        
        report_lines.extend([
            "",
            "## Policy Evolution",
            f"- High Risk Threshold: {self.policy_updater.policy.high_risk_threshold}",
            f"- Retry Multiplier: {self.policy_updater.policy.retry_multiplier}",
            f"- Priority Bias: {self.policy_updater.policy.priority_bias:.2f}",
            "",
            "## Safety & Compliance",
            "[OK] All high-risk tasks evaluated with strategic confidence elevation",
            "[OK] Safety gates enforced (high-risk threshold maintained)",
            "[OK] All executions remain dry-run",
            "[OK] Strategic decisions logged with rationale",
            "",
            "## Outputs",
            "- self_questions.jsonl",
            "- task_outcomes.jsonl",
            "- confidence_updates.jsonl",
            "- policy_updates.jsonl",
            "- strategic_decisions.jsonl",
            "- phase12_ui_state.json",
            "",
            "## Phase 13 Readiness",
            "[OK] Strategic decision-making validated",
            "[OK] Confidence elevation patterns established",
            "[OK] Policy adaptation operational",
            "[OK] Structured outputs ready for Phase 13"
        ])
        
        with open(self.output_dir / "PHASE_12_STRATEGIC_REPORT.md", "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))


def main():
    parser = argparse.ArgumentParser(description="Phase 12 Strategic Execution Harness")
    parser.add_argument("--waves", type=int, default=3, help="Number of waves to execute")
    parser.add_argument("--output-dir", type=str, default="outputs/phase12", help="Output directory")
    parser.add_argument("--phase11-dir", type=str, default="outputs/phase11", help="Phase 11 directory")
    
    args = parser.parse_args()
    
    harness = StrategicHarness(
        Path(args.phase11_dir),
        Path(args.output_dir)
    )
    harness.run(waves=args.waves)


if __name__ == "__main__":
    main()

